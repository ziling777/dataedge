#!/usr/bin/env python3
"""
ç”Ÿæˆ QuickSight çœ‹æ¿æ•°æ®çš„ EMR ä½œä¸š
ä» S3 Tables æŸ¥è¯¢æ•°æ®å¹¶ç”Ÿæˆèšåˆç»“æœä¿å­˜åˆ° S3
"""

import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

def main():
    if len(sys.argv) != 2:
        print("Usage: generate_dashboard_data.py <s3_bucket_name>")
        sys.exit(1)
    
    s3_bucket = sys.argv[1]
    
    # åˆ›å»º Spark ä¼šè¯
    spark = SparkSession.builder \
        .appName("GenerateDashboardData") \
        .getOrCreate()
    
    # S3 Tables é…ç½®
    canbus01_table = "gpdemo.greptime.canbus01"
    
    print(f"å¼€å§‹ç”Ÿæˆçœ‹æ¿æ•°æ®...")
    
    try:
        # è¯»å– S3 Tables æ•°æ®
        df = spark.table(canbus01_table)
        print(f"æˆåŠŸè¯»å–è¡¨: {canbus01_table}")
        print(f"æ•°æ®è¡Œæ•°: {df.count()}")
        
        # 1. ç”Ÿæˆå…³é”®æŒ‡æ ‡ç»Ÿè®¡
        print("ç”Ÿæˆå…³é”®æŒ‡æ ‡ç»Ÿè®¡...")
        key_metrics = spark.sql(f"""
            SELECT 
                'è½¦è¾†æ€»æ•°' as metric_name,
                COUNT(DISTINCT vin_id) as metric_value,
                'è¾†' as unit
            FROM {canbus01_table}
            
            UNION ALL
            
            SELECT 
                'æ€»è®°å½•æ•°' as metric_name,
                COUNT(*) as metric_value,
                'æ¡' as unit
            FROM {canbus01_table}
            
            UNION ALL
            
            SELECT 
                'å¹³å‡ç‡ƒæ²¹ç™¾åˆ†æ¯”' as metric_name,
                ROUND(AVG(fuel_percentage), 2) as metric_value,
                '%' as unit
            FROM {canbus01_table}
            
            UNION ALL
            
            SELECT 
                'å¹³å‡é€Ÿåº¦' as metric_name,
                ROUND(AVG(display_speed), 2) as metric_value,
                'km/h' as unit
            FROM {canbus01_table}
            
            UNION ALL
            
            SELECT 
                'ä½ç”µé‡è­¦å‘Š' as metric_name,
                SUM(CASE WHEN ress_power_low_flag = true THEN 1 ELSE 0 END) as metric_value,
                'æ¬¡' as unit
            FROM {canbus01_table}
            
            UNION ALL
            
            SELECT 
                'å¹³å‡CLTCç»­èˆª' as metric_name,
                ROUND(AVG(fuel_cltc_mileage), 2) as metric_value,
                'km' as unit
            FROM {canbus01_table}
            
            UNION ALL
            
            SELECT 
                'å¹³å‡ç›®æ ‡SOC' as metric_name,
                ROUND(AVG(target_soc), 2) as metric_value,
                '%' as unit
            FROM {canbus01_table}
        """)
        
        # ä¿å­˜å…³é”®æŒ‡æ ‡
        key_metrics_path = f"s3://{s3_bucket}/dashboard-data/key_metrics/"
        key_metrics.coalesce(1).write.mode("overwrite").option("header", "true").csv(key_metrics_path)
        print(f"å…³é”®æŒ‡æ ‡ä¿å­˜åˆ°: {key_metrics_path}")
        
        # 2. ç”Ÿæˆé©¾é©¶æ¨¡å¼åˆ†å¸ƒ
        print("ç”Ÿæˆé©¾é©¶æ¨¡å¼åˆ†å¸ƒ...")
        driving_modes = spark.sql(f"""
            SELECT 
                CONCAT('æ¸…æ´æ¨¡å¼', clean_mode) as mode_name,
                COUNT(*) as record_count,
                ROUND(AVG(fuel_percentage), 2) as avg_fuel,
                ROUND(AVG(display_speed), 2) as avg_speed,
                ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as percentage
            FROM {canbus01_table}
            GROUP BY clean_mode
            ORDER BY record_count DESC
        """)
        
        driving_modes_path = f"s3://{s3_bucket}/dashboard-data/driving_modes/"
        driving_modes.coalesce(1).write.mode("overwrite").option("header", "true").csv(driving_modes_path)
        print(f"é©¾é©¶æ¨¡å¼åˆ†å¸ƒä¿å­˜åˆ°: {driving_modes_path}")
        
        # 3. ç”Ÿæˆé€Ÿåº¦åˆ†å¸ƒ
        print("ç”Ÿæˆé€Ÿåº¦åˆ†å¸ƒ...")
        speed_distribution = spark.sql(f"""
            SELECT 
                CASE 
                    WHEN display_speed <= 20 THEN 'ä½é€Ÿ(â‰¤20km/h)'
                    WHEN display_speed <= 40 THEN 'ä¸­ä½é€Ÿ(21-40km/h)'
                    WHEN display_speed <= 60 THEN 'ä¸­é€Ÿ(41-60km/h)'
                    WHEN display_speed <= 80 THEN 'ä¸­é«˜é€Ÿ(61-80km/h)'
                    WHEN display_speed <= 100 THEN 'é«˜é€Ÿ(81-100km/h)'
                    ELSE 'è¶…é«˜é€Ÿ(>100km/h)'
                END as speed_category,
                COUNT(*) as record_count,
                ROUND(AVG(fuel_percentage), 2) as avg_fuel,
                ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as percentage
            FROM {canbus01_table}
            GROUP BY 
                CASE 
                    WHEN display_speed <= 20 THEN 'ä½é€Ÿ(â‰¤20km/h)'
                    WHEN display_speed <= 40 THEN 'ä¸­ä½é€Ÿ(21-40km/h)'
                    WHEN display_speed <= 60 THEN 'ä¸­é€Ÿ(41-60km/h)'
                    WHEN display_speed <= 80 THEN 'ä¸­é«˜é€Ÿ(61-80km/h)'
                    WHEN display_speed <= 100 THEN 'é«˜é€Ÿ(81-100km/h)'
                    ELSE 'è¶…é«˜é€Ÿ(>100km/h)'
                END
            ORDER BY record_count DESC
        """)
        
        speed_dist_path = f"s3://{s3_bucket}/dashboard-data/speed_distribution/"
        speed_distribution.coalesce(1).write.mode("overwrite").option("header", "true").csv(speed_dist_path)
        print(f"é€Ÿåº¦åˆ†å¸ƒä¿å­˜åˆ°: {speed_dist_path}")
        
        # 4. ç”Ÿæˆå……ç”µè¡Œä¸ºåˆ†æ
        print("ç”Ÿæˆå……ç”µè¡Œä¸ºåˆ†æ...")
        charging_behavior = spark.sql(f"""
            SELECT 
                CASE 
                    WHEN charging_time_remain_minute = 0 THEN 'æ— éœ€å……ç”µ'
                    WHEN charging_time_remain_minute <= 30 THEN 'å¿«é€Ÿå……ç”µ(â‰¤30åˆ†é’Ÿ)'
                    WHEN charging_time_remain_minute <= 120 THEN 'æ ‡å‡†å……ç”µ(31-120åˆ†é’Ÿ)'
                    ELSE 'é•¿æ—¶é—´å……ç”µ(>120åˆ†é’Ÿ)'
                END as charging_category,
                COUNT(*) as session_count,
                ROUND(AVG(target_soc), 2) as avg_target_soc,
                ROUND(AVG(fuel_percentage), 2) as avg_fuel_at_charging,
                ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as percentage
            FROM {canbus01_table}
            GROUP BY 
                CASE 
                    WHEN charging_time_remain_minute = 0 THEN 'æ— éœ€å……ç”µ'
                    WHEN charging_time_remain_minute <= 30 THEN 'å¿«é€Ÿå……ç”µ(â‰¤30åˆ†é’Ÿ)'
                    WHEN charging_time_remain_minute <= 120 THEN 'æ ‡å‡†å……ç”µ(31-120åˆ†é’Ÿ)'
                    ELSE 'é•¿æ—¶é—´å……ç”µ(>120åˆ†é’Ÿ)'
                END
            ORDER BY session_count DESC
        """)
        
        charging_path = f"s3://{s3_bucket}/dashboard-data/charging_behavior/"
        charging_behavior.coalesce(1).write.mode("overwrite").option("header", "true").csv(charging_path)
        print(f"å……ç”µè¡Œä¸ºåˆ†æä¿å­˜åˆ°: {charging_path}")
        
        # 5. ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
        print("ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®...")
        time_series = spark.sql(f"""
            SELECT 
                DATE_TRUNC('hour', ts) as hour_timestamp,
                COUNT(*) as record_count,
                COUNT(DISTINCT vin_id) as active_vehicles,
                ROUND(AVG(fuel_percentage), 2) as avg_fuel,
                ROUND(AVG(display_speed), 2) as avg_speed,
                SUM(CASE WHEN ress_power_low_flag = true THEN 1 ELSE 0 END) as low_power_alerts,
                SUM(CASE WHEN charging_time_remain_minute > 0 THEN 1 ELSE 0 END) as charging_sessions
            FROM {canbus01_table}
            GROUP BY DATE_TRUNC('hour', ts)
            ORDER BY hour_timestamp
        """)
        
        time_series_path = f"s3://{s3_bucket}/dashboard-data/time_series/"
        time_series.coalesce(1).write.mode("overwrite").option("header", "true").csv(time_series_path)
        print(f"æ—¶é—´åºåˆ—æ•°æ®ä¿å­˜åˆ°: {time_series_path}")
        
        print("âœ… æ‰€æœ‰çœ‹æ¿æ•°æ®ç”Ÿæˆå®Œæˆ!")
        print(f"æ•°æ®ä¿å­˜åœ¨: s3://{s3_bucket}/dashboard-data/")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æ•°æ®é¢„è§ˆ
        print("\nğŸ“Š å…³é”®æŒ‡æ ‡é¢„è§ˆ:")
        key_metrics.show(10, False)
        
        print("\nğŸ“ˆ é©¾é©¶æ¨¡å¼åˆ†å¸ƒé¢„è§ˆ:")
        driving_modes.show(10, False)
        
        print("\nğŸš— é€Ÿåº¦åˆ†å¸ƒé¢„è§ˆ:")
        speed_distribution.show(10, False)
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆçœ‹æ¿æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        raise e
    
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
